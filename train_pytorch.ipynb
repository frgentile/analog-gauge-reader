{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e478e097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d814a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset loader class, this will read the dataset folder and create a dataset object\n",
    "# where each element is a tuple of (image, [xc, yc, x1, y1, x2, y2, xv, yv]).\n",
    "# The image is a 3D tensor of shape (3, 256, 256) and the ground-truth is a list of 8 floats read from\n",
    "# the filename.\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        # List all image files in the directory\n",
    "        self.image_files = [f for f in os.listdir(root_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "        self.image_files.sort()  # Sort the files to ensure consistent ordering\n",
    "        # List of all ground-truth values\n",
    "        self.gt_files = [f for f in os.listdir(root_dir) if f.endswith('.txt')]\n",
    "        self.gt_files.sort()  # Sort the files to ensure consistent ordering\n",
    "\n",
    "        # Ensure that the dataset is not empty\n",
    "        if len(self.image_files) == 0:\n",
    "            raise ValueError(f\"No images found in the directory: {root_dir}\")\n",
    "        # Ensure that the ground-truth files match the images\n",
    "        if len(self.image_files) != len(self.gt_files):\n",
    "            raise ValueError(f\"Number of images ({len(self.image_files)}) does not match number of ground-truth files ({len(self.gt_files)}) in the directory: {root_dir}\")\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image\n",
    "        img_name = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        image = datasets.folder.default_loader(img_name)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        # Load the ground-truth\n",
    "        gt_name = os.path.join(self.root_dir, self.gt_files[idx])\n",
    "        with open(gt_name, 'r') as f:\n",
    "            gt_values = f.read().strip().split(',')\n",
    "            # Convert the ground-truth values to a list of floats\n",
    "            gt_values = torch.tensor([float(x) for x in gt_values], dtype=torch.float32)\n",
    "        # Ensure that the ground-truth values are in the correct format\n",
    "        if len(gt_values) != 8:\n",
    "            raise ValueError(f\"Ground-truth file {gt_name} does not contain 8 values.\")\n",
    "\n",
    "        return image, gt_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ead02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to view samples from the dataset\n",
    "def view_samples(dataset, num_samples=5):\n",
    "    net_img_size = 200\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(15, 5))\n",
    "    for i in range(num_samples):\n",
    "        idx = random.randint(0, len(dataset) - 1)\n",
    "        image, gt_values = dataset[idx]\n",
    "        axes[i].imshow(image.permute(1, 2, 0).numpy())\n",
    "        # Plot center\n",
    "        axes[i].plot(int(gt_values[0] * net_img_size), int(gt_values[1] * net_img_size), color='blue', marker='x', linewidth=2, markersize=12, markeredgewidth=4)\n",
    "        # Plot Mark 1 - Minimum at scale (x1, y1)\n",
    "        axes[i].plot(int(gt_values[2] * net_img_size), int(gt_values[3] * net_img_size), color='green', marker='x', linewidth=2, markersize=12, markeredgewidth=4)\n",
    "        # Plot Mark 2 - Maximum at scale (x2, y2)\n",
    "        axes[i].plot(int(gt_values[4] * net_img_size), int(gt_values[5] * net_img_size), color='yellow', marker='x', linewidth=2, markersize=12, markeredgewidth=4)\n",
    "        # Plot Gauge Mark - Point of the tail (xv, yv)\n",
    "        axes[i].plot(int(gt_values[6] * net_img_size), int(gt_values[7] * net_img_size), color='blue', marker='+', linewidth=2, markersize=12, markeredgewidth=4)        \n",
    "        axes[i].set_title(f\"idx: {idx}\")\n",
    "        axes[i].axis('off')\n",
    "        if i == num_samples - 1:\n",
    "            break\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a62572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "class GaugeValueEstimatorModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GaugeValueEstimatorModel, self).__init__()\n",
    "        # Backbone - Feature Extraction\n",
    "        # Output_Shape = (Input_Shape - Kernel_Size + 2Padding) / Stride + 1*\n",
    "        # Input: 3x200x200, Output: 64x200x200\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding='same')\n",
    "        # Input : 64x200x200, Output: 64x100x100\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        # Input: 64x100x100, Output: 128x100x100\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding='same')\n",
    "        # Input : 128x100x100, Output: 128x50x50\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        # Input: 128x50x50, Output: 256x50x50\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding='same')\n",
    "        # # Input : 64x200x200, Output: 64x200x200\n",
    "        # self.bn3 = nn.BatchNorm2d(64, momentum=0.99, eps=0.001)\n",
    "        # Input: 256x50x50, Output: 256x25x25\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        # Input: 256x25x25, Output: 512x25x25\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding='same')\n",
    "\n",
    "        # Head - Circle Parameters Regression\n",
    "        # Input: \n",
    "        self.fc1 = nn.Linear(512*25*25, 256) # Adjust input size based on the output of the backbone\n",
    "        self.fc2 = nn.Linear(256, 8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Backbone\n",
    "        x = F.relu(self.conv1(x))\n",
    "        # x = self.bn1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        # x = self.bn2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "        # x = self.bn3(x)\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = torch.flatten(x, start_dim=1)  # Flatten the output for the fully connected layer\n",
    "\n",
    "        # Head\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)  # Linear activation for regression\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc7a882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to train the model\n",
    "def train_model(device, model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, gt_values in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "            # Move data to the same device as the model\n",
    "            images = images.to(device)\n",
    "            gt_values = gt_values.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, gt_values)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Loss: {running_loss / len(train_loader)}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Define a function to test the model and return the predictions and ground-truth values and accuracy\n",
    "def test_model(device, model, test_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    total_loss = 0.0\n",
    "    criterion = nn.MSELoss()  # Define the loss function for accuracy calculation\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, gt_values in tqdm(test_loader, desc=\"Testing\"):\n",
    "            # Move data to the same device as the model\n",
    "            images = images.to(device)\n",
    "            gt_values = gt_values.to(device)\n",
    "\n",
    "            # Get model predictions\n",
    "            outputs = model(images)\n",
    "            predictions.append(outputs.cpu().numpy())\n",
    "            ground_truths.append(gt_values.cpu().numpy())\n",
    "\n",
    "            # Calculate loss for accuracy\n",
    "            loss = criterion(outputs, gt_values)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    # Calculate overall accuracy (mean squared error in this case)\n",
    "    accuracy = total_loss / len(test_loader)\n",
    "\n",
    "    return predictions, ground_truths, accuracy\n",
    "\n",
    "\n",
    "# Define a function to save the model\n",
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "\n",
    "# Define a function to load the model\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    print(f\"Model loaded from {path}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a330c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transformation for the dataset:\n",
    "dataset_transform = transforms.Compose([\n",
    "    transforms.Resize((200, 200)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "# Create a simple dataset\n",
    "full_dataset = CustomDataset(root_dir='dataset', transform=dataset_transform)\n",
    "# View samples from the dataset\n",
    "view_samples(full_dataset, num_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacd178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset to train and test\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c77182",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the model, define the loss function and the optimizer\n",
    "model = GaugeValueEstimatorModel()\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "criterion.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce625560",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d82e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model = train_model(device, model, train_loader, criterion, optimizer, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ad1fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "save_model(model, 'gauge_value_estimator.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa804c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = load_model(model, 'gauge_value_estimator.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171d1eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model and get accuracy\n",
    "predictions, gt, acc = test_model(device, model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d2059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e676a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
